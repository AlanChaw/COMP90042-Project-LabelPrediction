{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label with extracted evidence texts\n",
    "This notebook builds the MLP model for RTM step according to the FNC competition paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T05:51:13.516740Z",
     "start_time": "2019-05-19T05:51:10.934269Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_with_evi_text</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>The island 's geography comprises relatively l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling.</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling. ...</td>\n",
       "      <td>She made her film debut in the 2007 drama Dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>In 1932, Prussia was taken over.</td>\n",
       "      <td>In 1932, Prussia was taken over. ||| In the We...</td>\n",
       "      <td>In the Weimar Republic , the state of Prussia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IZombie premiered in 2015.</td>\n",
       "      <td>IZombie premiered in 2015. ||| The series prem...</td>\n",
       "      <td>The series premiered on March 17 , 2015 .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Reagan had a nationality.</td>\n",
       "      <td>Ronald Reagan had a nationality. ||| Ronald Wi...</td>\n",
       "      <td>Ronald Wilson Reagan February 6 , 1911 -- June...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Samoa Joe wrestles professionally.</td>\n",
       "      <td>Samoa Joe wrestles professionally. ||| Nuufola...</td>\n",
       "      <td>Nuufolau Joel `` Joe '' Seanoa born March 17 ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>University of Oxford is in the universe.</td>\n",
       "      <td>University of Oxford is in the universe. ||| T...</td>\n",
       "      <td>The University of Oxford informally Oxford Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Renaissance began online.</td>\n",
       "      <td>The Renaissance began online. ||| Their earlie...</td>\n",
       "      <td>Their earlier musical output was often labelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Portia de Rossi appeared on Scandal.</td>\n",
       "      <td>Portia de Rossi appeared on Scandal. ||| She a...</td>\n",
       "      <td>She appeared as a regular cast member on the A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years.</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years...</td>\n",
       "      <td>The Berlin Wall Berliner Mauer was a guarded c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                              claim  \\\n",
       "0       0    1    0  Ireland does not have relatively low-lying mou...   \n",
       "1       0    0    1      The drama Dark Matter stars Taylor Schilling.   \n",
       "2       0    0    1                   In 1932, Prussia was taken over.   \n",
       "3       0    0    1                         IZombie premiered in 2015.   \n",
       "4       0    0    1                   Ronald Reagan had a nationality.   \n",
       "5       0    0    1                 Samoa Joe wrestles professionally.   \n",
       "6       0    0    1           University of Oxford is in the universe.   \n",
       "7       1    0    0                      The Renaissance began online.   \n",
       "8       0    0    1               Portia de Rossi appeared on Scandal.   \n",
       "9       0    1    0    The Berlin Wall was only standing for 10 years.   \n",
       "\n",
       "                                 claim_with_evi_text  \\\n",
       "0  Ireland does not have relatively low-lying mou...   \n",
       "1  The drama Dark Matter stars Taylor Schilling. ...   \n",
       "2  In 1932, Prussia was taken over. ||| In the We...   \n",
       "3  IZombie premiered in 2015. ||| The series prem...   \n",
       "4  Ronald Reagan had a nationality. ||| Ronald Wi...   \n",
       "5  Samoa Joe wrestles professionally. ||| Nuufola...   \n",
       "6  University of Oxford is in the universe. ||| T...   \n",
       "7  The Renaissance began online. ||| Their earlie...   \n",
       "8  Portia de Rossi appeared on Scandal. ||| She a...   \n",
       "9  The Berlin Wall was only standing for 10 years...   \n",
       "\n",
       "                                            evi_text  \n",
       "0  The island 's geography comprises relatively l...  \n",
       "1  She made her film debut in the 2007 drama Dark...  \n",
       "2  In the Weimar Republic , the state of Prussia ...  \n",
       "3        The series premiered on March 17 , 2015 .\\n  \n",
       "4  Ronald Wilson Reagan February 6 , 1911 -- June...  \n",
       "5  Nuufolau Joel `` Joe '' Seanoa born March 17 ,...  \n",
       "6  The University of Oxford informally Oxford Uni...  \n",
       "7  Their earlier musical output was often labelle...  \n",
       "8  She appeared as a regular cast member on the A...  \n",
       "9  The Berlin Wall Berliner Mauer was a guarded c...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./JSONFiles/\" + \"train_with_text.json\"\n",
    "use_test_file = False\n",
    "if use_test_file:\n",
    "    test_file_path = './JSONFiles/' + 'test_with_text.json'\n",
    "else:\n",
    "    test_file_path = './JSONFiles/' + 'dev_with_text.json'\n",
    "\n",
    "with open(train_file_path, mode='r') as f:\n",
    "    train = json.load(f)\n",
    "with open(test_file_path, mode='r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "def load_training_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = ''.join(evi_texts)\n",
    "        if len(text) == 0:\n",
    "            text = \"no word\"\n",
    "\n",
    "        SUP = NOINFO = REF = 0\n",
    "        if record.get(\"label\") == \"SUPPORTS\":\n",
    "            SUP = 1\n",
    "        elif record.get(\"label\") == \"REFUTES\":\n",
    "            REF = 1\n",
    "        else:\n",
    "            NOINFO = 1\n",
    "        dataset_record = {\n",
    "            \"claim\": claim,\n",
    "            \"evi_text\": text,\n",
    "            \"claim_with_evi_text\": claim + \" ||| \" + text,\n",
    "            \"SUP\": SUP,\n",
    "            \"NOINFO\": NOINFO,\n",
    "            \"REF\": REF\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "def load_test_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_index = record.get(\"evidence\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = ''.join(evi_texts)\n",
    "        if len(text) == 0:\n",
    "            text = \"no word\"\n",
    "            \n",
    "        dataset_record = {\n",
    "            \"key\": key,\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evi_index,\n",
    "            \"claim_with_evi_text\": claim + \" ||| \" + text,\n",
    "            \"evi_text\": text\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "train_df = pd.DataFrame(load_training_data(train))\n",
    "test_df = pd.DataFrame(load_test_data(test))\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T05:51:20.758570Z",
     "start_time": "2019-05-19T05:51:20.742728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_with_evi_text</th>\n",
       "      <th>evi_text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ripon College's student number totaled in at a...</td>\n",
       "      <td>Ripon College's student number totaled in at a...</td>\n",
       "      <td>As of 2015 , Ripon College 's student body sto...</td>\n",
       "      <td>[[Ripon_College_-LRB-Wisconsin-RRB-, 1]]</td>\n",
       "      <td>100038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kesha was baptized on March 1st, 1987.</td>\n",
       "      <td>Kesha was baptized on March 1st, 1987. ||| Kes...</td>\n",
       "      <td>Kesha Rose Sebert ; born March 1 , 1987 ; form...</td>\n",
       "      <td>[[Kesha, 0]]</td>\n",
       "      <td>100083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birthday Song (2 Chainz song) was banned by So...</td>\n",
       "      <td>Birthday Song (2 Chainz song) was banned by So...</td>\n",
       "      <td>The song , which features fellow American rapp...</td>\n",
       "      <td>[[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]</td>\n",
       "      <td>100169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The University of Illinois at Chicago is a col...</td>\n",
       "      <td>The University of Illinois at Chicago is a col...</td>\n",
       "      <td>The University of Illinois at Chicago or UIC i...</td>\n",
       "      <td>[[University_of_Illinois_at_Chicago, 0]]</td>\n",
       "      <td>100234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French Indochina was officially known as the I...</td>\n",
       "      <td>French Indochina was officially known as the I...</td>\n",
       "      <td>Queen Square is the first element in `` the mo...</td>\n",
       "      <td>[[Queen_Square,_Bath, 1]]</td>\n",
       "      <td>100359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Damon Albarn has refused to ever work with Bri...</td>\n",
       "      <td>Damon Albarn has refused to ever work with Bri...</td>\n",
       "      <td>His debut solo studio album Everyday Robots --...</td>\n",
       "      <td>[[Damon_Albarn, 17]]</td>\n",
       "      <td>100366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lost (TV series) is a series of plays.</td>\n",
       "      <td>Lost (TV series) is a series of plays. ||| Los...</td>\n",
       "      <td>Lost is an American television drama series th...</td>\n",
       "      <td>[[Lost_-LRB-TV_series-RRB-, 0]]</td>\n",
       "      <td>100429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Edison Machine Works was barely set up to prod...</td>\n",
       "      <td>Edison Machine Works was barely set up to prod...</td>\n",
       "      <td>A metabibliography or biblio-bibliography is a...</td>\n",
       "      <td>[[Metabibliography, 0]]</td>\n",
       "      <td>100457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The human brain is set apart from mammalian br...</td>\n",
       "      <td>The human brain is set apart from mammalian br...</td>\n",
       "      <td>The office was replaced by the Lord Lieutenant...</td>\n",
       "      <td>[[Lord_Lieutenant_of_Ross-shire, 1]]</td>\n",
       "      <td>100461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There are rumors that Augustus' wife, Livia, p...</td>\n",
       "      <td>There are rumors that Augustus' wife, Livia, p...</td>\n",
       "      <td>Entire SH-09 between Dabok and Chittaurgarh ha...</td>\n",
       "      <td>[[Mavli, 41]]</td>\n",
       "      <td>100481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Ripon College's student number totaled in at a...   \n",
       "1             Kesha was baptized on March 1st, 1987.   \n",
       "2  Birthday Song (2 Chainz song) was banned by So...   \n",
       "3  The University of Illinois at Chicago is a col...   \n",
       "4  French Indochina was officially known as the I...   \n",
       "5  Damon Albarn has refused to ever work with Bri...   \n",
       "6             Lost (TV series) is a series of plays.   \n",
       "7  Edison Machine Works was barely set up to prod...   \n",
       "8  The human brain is set apart from mammalian br...   \n",
       "9  There are rumors that Augustus' wife, Livia, p...   \n",
       "\n",
       "                                 claim_with_evi_text  \\\n",
       "0  Ripon College's student number totaled in at a...   \n",
       "1  Kesha was baptized on March 1st, 1987. ||| Kes...   \n",
       "2  Birthday Song (2 Chainz song) was banned by So...   \n",
       "3  The University of Illinois at Chicago is a col...   \n",
       "4  French Indochina was officially known as the I...   \n",
       "5  Damon Albarn has refused to ever work with Bri...   \n",
       "6  Lost (TV series) is a series of plays. ||| Los...   \n",
       "7  Edison Machine Works was barely set up to prod...   \n",
       "8  The human brain is set apart from mammalian br...   \n",
       "9  There are rumors that Augustus' wife, Livia, p...   \n",
       "\n",
       "                                            evi_text  \\\n",
       "0  As of 2015 , Ripon College 's student body sto...   \n",
       "1  Kesha Rose Sebert ; born March 1 , 1987 ; form...   \n",
       "2  The song , which features fellow American rapp...   \n",
       "3  The University of Illinois at Chicago or UIC i...   \n",
       "4  Queen Square is the first element in `` the mo...   \n",
       "5  His debut solo studio album Everyday Robots --...   \n",
       "6  Lost is an American television drama series th...   \n",
       "7  A metabibliography or biblio-bibliography is a...   \n",
       "8  The office was replaced by the Lord Lieutenant...   \n",
       "9  Entire SH-09 between Dabok and Chittaurgarh ha...   \n",
       "\n",
       "                                       evidence     key  \n",
       "0      [[Ripon_College_-LRB-Wisconsin-RRB-, 1]]  100038  \n",
       "1                                  [[Kesha, 0]]  100083  \n",
       "2  [[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]  100169  \n",
       "3      [[University_of_Illinois_at_Chicago, 0]]  100234  \n",
       "4                     [[Queen_Square,_Bath, 1]]  100359  \n",
       "5                          [[Damon_Albarn, 17]]  100366  \n",
       "6               [[Lost_-LRB-TV_series-RRB-, 0]]  100429  \n",
       "7                       [[Metabibliography, 0]]  100457  \n",
       "8          [[Lord_Lieutenant_of_Ross-shire, 1]]  100461  \n",
       "9                                 [[Mavli, 41]]  100481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:20:53.400161Z",
     "start_time": "2019-05-17T07:19:50.031409Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_with_evi_text</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>moana be not a disney movie</td>\n",
       "      <td>Moana is not a Disney movie. ||| Moana -LRB- -...</td>\n",
       "      <td>moana lrb lsb moʊˈɑːnə rsb rrb be a 2016 ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>sweden be a country</td>\n",
       "      <td>Sweden is a country. ||| Sweden -LRB- Konungar...</td>\n",
       "      <td>sweden lrb konungariket sverige rrb be a scand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jeff hardy be an announcer</td>\n",
       "      <td>Jeff Hardy is an announcer. ||| Ziri Hammar -L...</td>\n",
       "      <td>ziri hammar lrb زيري حم ار bear july 25 1992 i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>francois de belleforest translate the work of ...</td>\n",
       "      <td>Francois de Belleforest translated the works o...</td>\n",
       "      <td>no word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>dominick dunne be involve in the film panic in...</td>\n",
       "      <td>Dominick Dunne was involved in the film Panic ...</td>\n",
       "      <td>he begin his career a a producer in film and t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>superior donut be rarely american</td>\n",
       "      <td>Superior Donuts is rarely American. ||| Superi...</td>\n",
       "      <td>superior donut be an american sitcom that air ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the kitti s hog nose bat belong to the chiropt...</td>\n",
       "      <td>The Kitti's hog-nosed bat belongs to the Chiro...</td>\n",
       "      <td>serhiy voronin a ukrainian footballer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>new horizon only fly past the smallest planet ...</td>\n",
       "      <td>New Horizons only flew past the smallest plane...</td>\n",
       "      <td>the jupiter flyby provide a gravity assist tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>stockard channing star in grease</td>\n",
       "      <td>Stockard Channing starred in Grease. ||| She i...</td>\n",
       "      <td>she be know for play betty rizzo in the film g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the house of lusignan reign during the middle age</td>\n",
       "      <td>The House of Lusignan reigned during the Middl...</td>\n",
       "      <td>the house of lusignan lrb lsb ˈluːzᵻnjɒn rsb r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                              claim  \\\n",
       "0       0    1    0                        moana be not a disney movie   \n",
       "1       0    0    1                                sweden be a country   \n",
       "2       1    0    0                         jeff hardy be an announcer   \n",
       "3       0    0    1  francois de belleforest translate the work of ...   \n",
       "4       0    0    1  dominick dunne be involve in the film panic in...   \n",
       "5       0    1    0                  superior donut be rarely american   \n",
       "6       1    0    0  the kitti s hog nose bat belong to the chiropt...   \n",
       "7       0    1    0  new horizon only fly past the smallest planet ...   \n",
       "8       0    0    1                   stockard channing star in grease   \n",
       "9       0    0    1  the house of lusignan reign during the middle age   \n",
       "\n",
       "                                 claim_with_evi_text  \\\n",
       "0  Moana is not a Disney movie. ||| Moana -LRB- -...   \n",
       "1  Sweden is a country. ||| Sweden -LRB- Konungar...   \n",
       "2  Jeff Hardy is an announcer. ||| Ziri Hammar -L...   \n",
       "3  Francois de Belleforest translated the works o...   \n",
       "4  Dominick Dunne was involved in the film Panic ...   \n",
       "5  Superior Donuts is rarely American. ||| Superi...   \n",
       "6  The Kitti's hog-nosed bat belongs to the Chiro...   \n",
       "7  New Horizons only flew past the smallest plane...   \n",
       "8  Stockard Channing starred in Grease. ||| She i...   \n",
       "9  The House of Lusignan reigned during the Middl...   \n",
       "\n",
       "                                            evi_text  \n",
       "0  moana lrb lsb moʊˈɑːnə rsb rrb be a 2016 ameri...  \n",
       "1  sweden lrb konungariket sverige rrb be a scand...  \n",
       "2  ziri hammar lrb زيري حم ار bear july 25 1992 i...  \n",
       "3                                            no word  \n",
       "4  he begin his career a a producer in film and t...  \n",
       "5  superior donut be an american sitcom that air ...  \n",
       "6              serhiy voronin a ukrainian footballer  \n",
       "7  the jupiter flyby provide a gravity assist tha...  \n",
       "8  she be know for play betty rizzo in the film g...  \n",
       "9  the house of lusignan lrb lsb ˈluːzᵻnjɒn rsb r...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    # remove stop words\n",
    "#     stop_words = nltk.corpus.stopwords.words('english')\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    # return result\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    corpus = pd.concat([dataset['claim'], dataset['evi_text']])\n",
    "    processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "    dataset['claim'] = processed_corpus.iloc[0: len(dataset)]\n",
    "    dataset['evi_text'] = processed_corpus.iloc[len(dataset):,]\n",
    "    return dataset\n",
    "\n",
    "train_df = process_dataset(train_df)\n",
    "test_df = process_dataset(test_df)\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T05:51:48.982625Z",
     "start_time": "2019-05-19T05:51:48.974984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Ireland does not have relatively low-lying mou...\n",
       "1             The drama Dark Matter stars Taylor Schilling.\n",
       "2                          In 1932, Prussia was taken over.\n",
       "3                                IZombie premiered in 2015.\n",
       "4                          Ronald Reagan had a nationality.\n",
       "5                        Samoa Joe wrestles professionally.\n",
       "6                  University of Oxford is in the universe.\n",
       "7                             The Renaissance began online.\n",
       "8                      Portia de Rossi appeared on Scandal.\n",
       "9           The Berlin Wall was only standing for 10 years.\n",
       "10                               Laurie Hernandez competes.\n",
       "11        There are zero cities in the Northeast megalop...\n",
       "12                         There is a film called Zootopia.\n",
       "13                 Portia de Rossi was featured on Scandal.\n",
       "14                       Sean Connery was cast in The Rock.\n",
       "15        San Francisco is the location of the Hudson Br...\n",
       "16        Dennis Quaid decided to act in any film but a ...\n",
       "17                              Unapologetic is by Rihanna.\n",
       "18                    Laurie Hernandez's cat's name is Zoe.\n",
       "19                           Chris Froome is not a cyclist.\n",
       "20        Lewis Hamilton was named Food Personality of t...\n",
       "21            One of the four terrestrial planets is Venus.\n",
       "22                 The Comedian featured an American actor.\n",
       "23        The Olympic Games are not  international sport...\n",
       "24                          Carbon has the atomic number 6.\n",
       "25                                 Ellyse Perry is Chinese.\n",
       "26                                   Julianne Hough dances.\n",
       "27        Gilmore Girls was executive produced by Daniel...\n",
       "28                In 2001, Alkaline Trio released an album.\n",
       "29        Big Bang won 7 awards at the 9th MTV Video Mus...\n",
       "                                ...                        \n",
       "145419                        No Escape had pre-screenings.\n",
       "145420                Kristen Bell has been on an airplane.\n",
       "145421                      Sage Stallone was only British.\n",
       "145422       A National Film Award was won by Chokher Bali.\n",
       "145423    The population growth of Las Vegas has acceler...\n",
       "145424                           Saamy is a melodrama film.\n",
       "145425    Betty Buckley has received a Grammy Award nomi...\n",
       "145426           Lymph nodes are major B lymphocytes sites.\n",
       "145427        Paul Thomas Anderson worked with Emma Watson.\n",
       "145428    Shahid Kapoor has collaborated with Vishal Bha...\n",
       "145429    Nelson Mandela introduced healthcare service e...\n",
       "145430     Linkin Park is an American band that plays rock.\n",
       "145431                            Emma Roberts is a singer.\n",
       "145432                  Jean-Claude Van Damme is a dentist.\n",
       "145433    Bill Gates is rarely the co-founder of Microsoft.\n",
       "145434    The Great Gatsby centers on a youthful person ...\n",
       "145435                          Kid Rock released a single.\n",
       "145436                                  Emma Roberts sings.\n",
       "145437         Henrietta Maria of France died on August 10.\n",
       "145438    Marie Curie conducted pioneering radioactivity...\n",
       "145439                              Kurt Angle is a German.\n",
       "145440                           Mammals are grave diggers.\n",
       "145441                   Chris Froome's parents are Indian.\n",
       "145442       The Simpsons have aired on Fox for 29 seasons.\n",
       "145443    Another One Bites the Dust was on Queen's set ...\n",
       "145444    William Shatner did not host a show that won a...\n",
       "145445         Pablo Fenjves ghostwrote for a acting agent.\n",
       "145446    The Republic of China was recognized as China'...\n",
       "145447                           Daz Dillinger is a lawyer.\n",
       "145448                Jesus did not begin his own ministry.\n",
       "Name: claim, Length: 145449, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:21:07.116887Z",
     "start_time": "2019-05-17T07:20:53.402438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "max_features = 5000 \n",
    "\n",
    "train_corpus = pd.concat([train_df['claim'], train_df['evi_text']])\n",
    "test_corpus = pd.concat([test_df['claim'], test_df['evi_text']])\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "tf_vectorizer.fit(train_corpus)\n",
    "train_claim_tf_features = tf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tf_features = tf_vectorizer.transform(train_df['evi_text'])\n",
    "test_claim_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "\n",
    "train_tf_features = hstack([train_claim_tf_features, train_evi_tf_features])\n",
    "test_tf_features = hstack([test_claim_tf_features, test_evi_tf_features])\n",
    "# claim_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# claim_tf = claim_tf_vectorizer.fit_transform(train_df['claim'])\n",
    "# evi_text_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# evi_text_tf = evi_text_tf_vectorizer.fit_transform(train_df['evi_text'])\n",
    "# tf_features = hstack([claim_tf, evi_text_tf])\n",
    "\n",
    "# tf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:22:09.715671Z",
     "start_time": "2019-05-17T07:21:07.118680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "all_corpus = pd.concat([train_corpus, test_corpus])\n",
    "\n",
    "def calculate_cosines(claim_tfidf, evi_tfidf) -> np.ndarray:\n",
    "    cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "    for i in range(len(cosines)):\n",
    "        claim_vector = claim_tfidf[i]\n",
    "        evi_vector = evi_tfidf[i]\n",
    "        cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "        cosines[i][0] = cosine_matrix[0][1]\n",
    "    return cosines\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "tfidf_vectorizer.fit(all_corpus)\n",
    "\n",
    "train_claim_tfidf = tfidf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tfidf = tfidf_vectorizer.transform(train_df['evi_text'])\n",
    "train_cosines = calculate_cosines(train_claim_tfidf, train_evi_tfidf)\n",
    "\n",
    "test_claim_tfidf = tfidf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tfidf = tfidf_vectorizer.transform(test_df['evi_text'])\n",
    "test_cosines = calculate_cosines(test_claim_tfidf, test_evi_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:25:00.223600Z",
     "start_time": "2019-05-04T09:25:00.220788Z"
    }
   },
   "source": [
    "#### Concat features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:22:13.822276Z",
     "start_time": "2019-05-17T07:22:09.717493Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145449, 10001)\n",
      "(145449, 3)\n",
      "(5001, 10001)\n"
     ]
    }
   ],
   "source": [
    "x_train = hstack([train_tf_features, train_cosines]).toarray()\n",
    "y_train = train_df[train_df.columns[0:3]].values\n",
    "x_test = hstack([test_tf_features, test_cosines]).toarray()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train model\n",
    "Build an MLP with tensor (10001, 1) as input, 1 hidden layer with 100 neurons, and softmax layer for output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP model prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.037134Z",
     "start_time": "2019-05-17T07:22:13.824111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1000200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 130904 samples, validate on 14545 samples\n",
      "Epoch 1/50\n",
      "130904/130904 [==============================] - 9s 67us/step - loss: 0.5294 - acc: 0.7844 - val_loss: 0.3864 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84428, saving model to best_weights.hdf5\n",
      "Epoch 2/50\n",
      "130904/130904 [==============================] - 7s 56us/step - loss: 0.3834 - acc: 0.8449 - val_loss: 0.3572 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84428 to 0.85734, saving model to best_weights.hdf5\n",
      "Epoch 3/50\n",
      "130904/130904 [==============================] - 7s 55us/step - loss: 0.3447 - acc: 0.8588 - val_loss: 0.3504 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.85734 to 0.85871, saving model to best_weights.hdf5\n",
      "Epoch 4/50\n",
      "130904/130904 [==============================] - 7s 54us/step - loss: 0.3224 - acc: 0.8679 - val_loss: 0.3495 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85871 to 0.86188, saving model to best_weights.hdf5\n",
      "Epoch 5/50\n",
      "130904/130904 [==============================] - 7s 55us/step - loss: 0.3047 - acc: 0.8757 - val_loss: 0.3467 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.86188 to 0.86449, saving model to best_weights.hdf5\n",
      "Epoch 6/50\n",
      "130904/130904 [==============================] - 8s 59us/step - loss: 0.2900 - acc: 0.8815 - val_loss: 0.3470 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe622d96320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "# {lr=0.01, batch_size=128, dropout=0.5, units=100}\n",
    "# {lr=0.001, batch_size=256, dropout=0.6, units=100}\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# callbacks\n",
    "filepath=\"best_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=1, verbose=0, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=50, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tune hyper-parameters mannually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.054367Z",
     "start_time": "2019-05-17T07:23:56.043285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# class Cartesian(object):\n",
    "#     def __init__(self):\n",
    "#         self._data_list = []\n",
    "#         self._name_list = []\n",
    "#         self.cartesian_result = []\n",
    "\n",
    "#     def add_data(self, data, name): #add list for cartesian product\n",
    "#         self._data_list.append(data)\n",
    "#         self._name_list.append(name)\n",
    "\n",
    "#     def build(self): #calculate cartesian product\n",
    "#         for item in itertools.product(*self._data_list):\n",
    "#             result_dict = {}\n",
    "#             for i in range(len(item)):\n",
    "#                 result_dict.update({\n",
    "#                     self._name_list[i]: item[i]\n",
    "#                 })\n",
    "#             self.cartesian_result.append(result_dict)\n",
    "#         return self.cartesian_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.073601Z",
     "start_time": "2019-05-17T07:23:56.057559Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# # from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from keras.optimizers import Adam\n",
    "# import keras\n",
    "\n",
    "# # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def create_model(units = 100, dropout = 0.5, lr = 0.001):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=units, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     optimizer = Adam(lr=lr)\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def fit_model(model, batch_size=32):\n",
    "#     earlyStopping = EarlyStopping(monitor='val_acc', patience=3, \n",
    "#                                   verbose=0, mode='auto')\n",
    "#     callbacks_list = [earlyStopping]\n",
    "\n",
    "#     model_history = model.fit(x=x_train, y=y_train, \n",
    "#                               batch_size=batch_size, epochs=50, \n",
    "#                               validation_split=0.1, callbacks=callbacks_list, verbose=1)\n",
    "#     return model_history\n",
    "\n",
    "# units_list = [25, 50, 100, 250, 500]\n",
    "# dropout_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# batch_size_list = [16, 32, 64, 128, 256, 512]\n",
    "# lr_list = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "# car_product=Cartesian()\n",
    "# car_product.add_data(units_list, 'units')\n",
    "# car_product.add_data(dropout_list, 'dropout')\n",
    "# car_product.add_data(batch_size_list, 'batch_size')\n",
    "# car_product.add_data(lr_list, 'lr')\n",
    "# parameter_combinations = car_product.build()\n",
    "\n",
    "# historys_list = []\n",
    "# iternum = 0\n",
    "# for combination in parameter_combinations:\n",
    "#     print(\"itertion: \" + str(iternum))\n",
    "#     print(combination)\n",
    "#     model = create_model(units=combination['units'], \n",
    "#                          dropout=combination['dropout'], \n",
    "#                          lr=combination['lr'])\n",
    "#     model_history = fit_model(model=model, batch_size=combination['batch_size'])\n",
    "#     historys_list.append({\n",
    "#         'combination': combination,\n",
    "#         'max_val_acc': max(model_history.history['val_acc'])\n",
    "#     })\n",
    "#     print(\"result: \" + str(max(model_history.history['val_acc'])))\n",
    "#     iternum += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.080040Z",
     "start_time": "2019-05-17T07:23:56.076260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # sort and output_to_file\n",
    "# ordered_history = sorted(historys_list, key= lambda x: x['max_val_acc'], reverse=True)\n",
    "\n",
    "# historys_list_dict = {\n",
    "#     \"historys\": ordered_history\n",
    "# }\n",
    "# with open('tune_hps.json', 'w') as hp_result:\n",
    "#     json.dump(ordered_history, hp_result, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tune hyper-parameters with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.091078Z",
     "start_time": "2019-05-17T07:23:56.082258Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "# # # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def create_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "# batch_size = [64, 128]\n",
    "# # epochs = [1, 2]\n",
    "# param_grid = dict(batch_size=batch_size)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2)\n",
    "# grid_result = grid.fit(X=x_train, y=y_train)\n",
    "\n",
    "\n",
    "# # summarize results\n",
    "# # print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# # means = grid_result.cv_results_['mean_test_score']\n",
    "# # stds = grid_result.cv_results_['std_test_score']\n",
    "# # params = grid_result.cv_results_['params']\n",
    "# # for mean, stdev, param in zip(means, stds, params):\n",
    "# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:09:57.964156Z",
     "start_time": "2019-05-16T05:09:57.958931Z"
    },
    "cell_style": "center",
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.097294Z",
     "start_time": "2019-05-17T07:23:56.092933Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'], verbose=2)\n",
    "#     return model\n",
    "\n",
    "# def fit_model():\n",
    "#     # callbacks\n",
    "#     filepath=\"best_weights.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#     earlyStopping = EarlyStopping(monitor='val_acc', patience=1, verbose=0, mode='min')\n",
    "\n",
    "#     callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "#     model.fit(x=x_train, y=y_train, batch_size=128, epochs=10, validation_split=0.1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T10:39:45.256150Z",
     "start_time": "2019-04-29T10:39:45.252494Z"
    }
   },
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.924704Z",
     "start_time": "2019-05-17T07:23:56.099079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001/5001 [==============================] - 0s 45us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.7777557e-05, 8.1297863e-01, 1.8694350e-01],\n",
       "       [1.3373666e-04, 9.9978524e-01, 8.1002356e-05],\n",
       "       [3.7203248e-05, 1.3802871e-01, 8.6193407e-01],\n",
       "       ...,\n",
       "       [1.3517000e-04, 2.7017993e-01, 7.2968489e-01],\n",
       "       [9.9983287e-01, 9.6332136e-05, 7.0762784e-05],\n",
       "       [1.9827129e-01, 6.0392892e-01, 1.9779973e-01]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"best_weights.hdf5\")\n",
    "y_test = model.predict(x_test, batch_size=128, verbose=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:38:04.076572Z",
     "start_time": "2019-05-05T07:38:04.073683Z"
    }
   },
   "source": [
    "### Output result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:57.350551Z",
     "start_time": "2019-05-17T07:23:56.926622Z"
    }
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    if np.argmax(y_test[i]) == 0:\n",
    "        label = \"NOT ENOUGH INFO\"\n",
    "    elif np.argmax(y_test[i]) == 1:\n",
    "        label = \"REFUTES\"\n",
    "    else:\n",
    "        label = \"SUPPORTS\"\n",
    "    key = test_df['key'][i]\n",
    "    result_dict.update({\n",
    "        key:{\n",
    "            \"claim\": test_df['claim'][i],\n",
    "            \"label\": label,\n",
    "            \"evidence\": test_df['evidence'][i]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "with open('result_on_dev.json', 'w') as outfile:\n",
    "    json.dump(result_dict, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

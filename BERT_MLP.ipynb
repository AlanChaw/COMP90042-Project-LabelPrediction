{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction website: https://bert-as-service.readthedocs.io/en/latest/section/get-start.html  \n",
    "Download server and client:\n",
    "``` bash\n",
    "pip install -U bert-serving-server bert-serving-client  \n",
    "```\n",
    "Downlaod and unzip pretrained bert model(BERT-Large, Uncased, 1024 dimensional output):  \n",
    "``` bash\n",
    "cd ${model_path}\n",
    "wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
    "unzip uncased_L-24_H-1024_A-16.zip  \n",
    "```  \n",
    "\n",
    "Start bert server at local machine: \n",
    "``` bash\n",
    "bert-serving-start -model_dir ${model_path}/uncased_L-24_H-1024_A-16 -max_seq_len=100 -num_worker=1  \n",
    "bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "```\n",
    "Then, call from client end in python:\n",
    "``` python\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "bc.encode(['First do it', 'then do it right', 'then do it better'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:43:47.817994Z",
     "start_time": "2019-05-17T05:43:47.813975Z"
    }
   },
   "source": [
    "## Load data as Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T09:58:13.767265Z",
     "start_time": "2019-05-20T09:58:11.609715Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_with_evi_text</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>Ireland The island 's geography comprises rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling.</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling. ...</td>\n",
       "      <td>Taylor Schilling She made her film debut in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>In 1932, Prussia was taken over.</td>\n",
       "      <td>In 1932, Prussia was taken over. ||| Prussia I...</td>\n",
       "      <td>Prussia In the Weimar Republic , the state of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IZombie premiered in 2015.</td>\n",
       "      <td>IZombie premiered in 2015. ||| IZombie TV seri...</td>\n",
       "      <td>IZombie TV series The series premiered on Marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Reagan had a nationality.</td>\n",
       "      <td>Ronald Reagan had a nationality. ||| Ronald Re...</td>\n",
       "      <td>Ronald Reagan Ronald Wilson Reagan February 6 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Samoa Joe wrestles professionally.</td>\n",
       "      <td>Samoa Joe wrestles professionally. ||| Samoa J...</td>\n",
       "      <td>Samoa Joe Nuufolau Joel `` Joe '' Seanoa born ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>University of Oxford is in the universe.</td>\n",
       "      <td>University of Oxford is in the universe. ||| U...</td>\n",
       "      <td>University of Oxford The University of Oxford ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Renaissance began online.</td>\n",
       "      <td>The Renaissance began online. ||| Mulankunnath...</td>\n",
       "      <td>Mulankunnathukavu railway station Two Shornur-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Portia de Rossi appeared on Scandal.</td>\n",
       "      <td>Portia de Rossi appeared on Scandal. ||| Porti...</td>\n",
       "      <td>Portia de Rossi She appeared as a regular cast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years.</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years...</td>\n",
       "      <td>Berlin Wall The Berlin Wall Berliner Mauer was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                              claim  \\\n",
       "0       0    1    0  Ireland does not have relatively low-lying mou...   \n",
       "1       0    0    1      The drama Dark Matter stars Taylor Schilling.   \n",
       "2       0    0    1                   In 1932, Prussia was taken over.   \n",
       "3       0    0    1                         IZombie premiered in 2015.   \n",
       "4       0    0    1                   Ronald Reagan had a nationality.   \n",
       "5       0    0    1                 Samoa Joe wrestles professionally.   \n",
       "6       0    0    1           University of Oxford is in the universe.   \n",
       "7       1    0    0                      The Renaissance began online.   \n",
       "8       0    0    1               Portia de Rossi appeared on Scandal.   \n",
       "9       0    1    0    The Berlin Wall was only standing for 10 years.   \n",
       "\n",
       "                                 claim_with_evi_text  \\\n",
       "0  Ireland does not have relatively low-lying mou...   \n",
       "1  The drama Dark Matter stars Taylor Schilling. ...   \n",
       "2  In 1932, Prussia was taken over. ||| Prussia I...   \n",
       "3  IZombie premiered in 2015. ||| IZombie TV seri...   \n",
       "4  Ronald Reagan had a nationality. ||| Ronald Re...   \n",
       "5  Samoa Joe wrestles professionally. ||| Samoa J...   \n",
       "6  University of Oxford is in the universe. ||| U...   \n",
       "7  The Renaissance began online. ||| Mulankunnath...   \n",
       "8  Portia de Rossi appeared on Scandal. ||| Porti...   \n",
       "9  The Berlin Wall was only standing for 10 years...   \n",
       "\n",
       "                                            evi_text  \n",
       "0  Ireland The island 's geography comprises rela...  \n",
       "1  Taylor Schilling She made her film debut in th...  \n",
       "2  Prussia In the Weimar Republic , the state of ...  \n",
       "3  IZombie TV series The series premiered on Marc...  \n",
       "4  Ronald Reagan Ronald Wilson Reagan February 6 ...  \n",
       "5  Samoa Joe Nuufolau Joel `` Joe '' Seanoa born ...  \n",
       "6  University of Oxford The University of Oxford ...  \n",
       "7  Mulankunnathukavu railway station Two Shornur-...  \n",
       "8  Portia de Rossi She appeared as a regular cast...  \n",
       "9  Berlin Wall The Berlin Wall Berliner Mauer was...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./JSONFiles/\" + \"train_with_text.json\"\n",
    "use_test_file = True\n",
    "if use_test_file:\n",
    "    test_file_path = './JSONFiles/' + 'result_test_521_tfidf080_text.json'\n",
    "else:\n",
    "    test_file_path = './JSONFiles/' + 'dev_with_text.json'\n",
    "\n",
    "with open(train_file_path, mode='r') as f:\n",
    "    train = json.load(f)\n",
    "with open(test_file_path, mode='r') as f:\n",
    "    test = json.load(f)\n",
    "    \n",
    "result_dict = {}\n",
    "\n",
    "def load_training_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    sorted_keys = sorted(dataset.keys())\n",
    "    for key in sorted_keys:\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = ''.join(evi_texts)\n",
    "        if len(text) == 0 or text == '\\n':\n",
    "            text = \"no word\"\n",
    "\n",
    "        SUP = NOINFO = REF = 0\n",
    "        if record.get(\"label\") == \"SUPPORTS\":\n",
    "            SUP = 1\n",
    "        elif record.get(\"label\") == \"REFUTES\":\n",
    "            REF = 1\n",
    "        else:\n",
    "            NOINFO = 1\n",
    "        dataset_record = {\n",
    "            \"claim\": claim,\n",
    "            \"evi_text\": text,\n",
    "            \"claim_with_evi_text\": claim + \" ||| \" + text,\n",
    "            \"SUP\": SUP,\n",
    "            \"NOINFO\": NOINFO,\n",
    "            \"REF\": REF\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "def load_test_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    sorted_keys = sorted(dataset.keys())\n",
    "    for key in sorted_keys:\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_index = record.get(\"evidence\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = ''.join(evi_texts)\n",
    "        \n",
    "        if len(evi_index) == 0 or text == '\\n':\n",
    "            result_dict.update({\n",
    "                key:{\n",
    "                    \"claim\": claim,\n",
    "                    \"label\": \"NOT ENOUGH INFO\",\n",
    "                    \"evidence\": []\n",
    "                    }\n",
    "            })\n",
    "            continue\n",
    "        if len(text) == 0:\n",
    "            text = \"no word\"\n",
    "            \n",
    "        dataset_record = {\n",
    "            \"key\": key,\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evi_index,\n",
    "            \"claim_with_evi_text\": claim + \" ||| \" + text,\n",
    "            \"evi_text\": text\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "train_df = pd.DataFrame(load_training_data(train))\n",
    "test_df = pd.DataFrame(load_test_data(test))\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11879"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T10:22:02.299532Z",
     "start_time": "2019-05-17T10:22:02.296894Z"
    }
   },
   "source": [
    "### Construct and save bert features to file for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T10:22:01.605677Z",
     "start_time": "2019-05-17T08:53:06.342210Z"
    }
   },
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "\n",
    "# For uncased\n",
    "# restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_batch_size=384 -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "# For cased\n",
    "# restart server with\n",
    "# bert-serving-start -model_dir /share/ShareFolder/cased_L-24_H-1024_A-16/ -cased_tokenization -max_batch_size=384 -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# case = \"\"\n",
    "case = \"_cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test claim encode\n",
    "# restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_seq_len=50 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "\n",
    "# train_claim_encode = bc.encode(list(train_df['claim']))\n",
    "# np.save(\"./BERT_MLP_encodings/train_claim_encode\" + case, train_claim_encode)\n",
    "\n",
    "test_claim_encode = bc.encode(list(test_df['claim']))\n",
    "np.save(\"./BERT_MLP_encodings/test_claim_encode\" + case, test_claim_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test evidence encode\n",
    "# restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "# train_evi_encode = bc.encode(list(train_df['evi_text']))\n",
    "# np.save(\"./BERT_MLP_encodings/train_evi_encode\" + case, train_evi_encode)\n",
    "\n",
    "test_evi_encode = bc.encode(list(test_df['evi_text']))\n",
    "np.save(\"./BERT_MLP_encodings/test_evi_encode\" + case, test_evi_encode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test claim+evidence pair encode\n",
    "# restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "\n",
    "# train_pair_encode = bc.encode(list(train_df['claim_with_evi_text']))\n",
    "# np.save(\"./BERT_MLP_encodings/train_pair_encode\" + case, train_pair_encode)\n",
    "\n",
    "test_pair_encode = bc.encode(list(test_df['claim_with_evi_text']))\n",
    "np.save(\"./BERT_MLP_encodings/test_pair_encode\" + case, test_pair_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bert features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_claim_features = np.load(\"./BERT_MLP_encodings/train_claim_encode.npy\")\n",
    "test_claim_features = np.load(\"./BERT_MLP_encodings/test_claim_encode.npy\")\n",
    "\n",
    "train_evi_features = np.load(\"./BERT_MLP_encodings/train_evi_encode.npy\")\n",
    "test_evi_features = np.load(\"./BERT_MLP_encodings/test_evi_encode.npy\")\n",
    "\n",
    "train_pair_features = np.load(\"./BERT_MLP_encodings/train_pair_encode.npy\")\n",
    "test_pair_features = np.load(\"././BERT_MLP_encodings/test_pair_encode.npy\")\n",
    "\n",
    "train_claim_features_cased = np.load(\"./BERT_MLP_encodings/train_claim_encode_cased.npy\")\n",
    "test_claim_features_cased = np.load(\"./BERT_MLP_encodings/test_claim_encode_cased.npy\")\n",
    "\n",
    "train_evi_features_cased = np.load(\"./BERT_MLP_encodings/train_evi_encode_cased.npy\")\n",
    "test_evi_features_cased = np.load(\"./BERT_MLP_encodings/test_evi_encode_cased.npy\")\n",
    "\n",
    "train_pair_features_cased = np.load(\"./BERT_MLP_encodings/train_pair_encode_cased.npy\")\n",
    "test_pair_features_cased = np.load(\"././BERT_MLP_encodings/test_pair_encode_cased.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([train_claim_features, train_evi_features, train_pair_features, train_claim_features_cased, train_evi_features_cased, train_pair_features_cased], axis=1)\n",
    "y_train = train_df[train_df.columns[0:3]].values\n",
    "x_test = np.concatenate([test_claim_features, test_evi_features, test_pair_features, test_claim_features_cased, test_evi_features_cased, test_pair_features_cased], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145449, 6144)\n",
      "(145449, 3)\n",
      "(11879, 6144)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP model prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 200)               1229000   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 1,239,203\n",
      "Trainable params: 1,239,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=200, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(units=50, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "# optimizer = Adam(lr=0.01)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# callbacks\n",
    "filepath=\"best_weights_head.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=3, verbose=0, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "# model.fit(x=x_train, y=y_train, batch_size=32, epochs=50, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Hyperparameters mannually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# class Cartesian(object):\n",
    "#     def __init__(self):\n",
    "#         self._data_list = []\n",
    "#         self._name_list = []\n",
    "#         self.cartesian_result = []\n",
    "\n",
    "#     def add_data(self, data, name): #add list for cartesian product\n",
    "#         self._data_list.append(data)\n",
    "#         self._name_list.append(name)\n",
    "\n",
    "#     def build(self): #calculate cartesian product\n",
    "#         for item in itertools.product(*self._data_list):\n",
    "#             result_dict = {}\n",
    "#             for i in range(len(item)):\n",
    "#                 result_dict.update({\n",
    "#                     self._name_list[i]: item[i]\n",
    "#                 })\n",
    "#             self.cartesian_result.append(result_dict)\n",
    "#         return self.cartesian_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# # from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.regularizers import l2\n",
    "# import keras\n",
    "\n",
    "# # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def create_model(first_units = 200, second_units = 50, dropout = 0.5, decay = 0.01):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=first_units, activation='relu',\n",
    "#                     kernel_regularizer=l2(decay),\n",
    "#                     input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(units=second_units, activation='relu', \n",
    "#                     kernel_regularizer=l2(decay),\n",
    "#                     input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "# #     optimizer = Adam(lr=0.01)\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def fit_model(model, batch_size=32):\n",
    "#     earlyStopping = EarlyStopping(monitor='val_acc', patience=2, \n",
    "#                                   verbose=0, mode='auto')\n",
    "#     callbacks_list = [earlyStopping]\n",
    "\n",
    "#     model_history = model.fit(x=x_train, y=y_train, \n",
    "#                               batch_size=batch_size, epochs=50, \n",
    "#                               validation_split=0.15, callbacks=callbacks_list, verbose=0)\n",
    "#     return model_history\n",
    "\n",
    "# first_units_list = [100, 250, 300, 500]\n",
    "# second_units_list = [50, 100, 150, 200]\n",
    "# dropout_list = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# decay_list = [0.00001, 0.0001, 0.001]\n",
    "    \n",
    "# car_product=Cartesian()\n",
    "# car_product.add_data(first_units_list, 'first_units')\n",
    "# car_product.add_data(second_units_list, 'second_units')\n",
    "# car_product.add_data(dropout_list, 'dropout')\n",
    "# car_product.add_data(decay_list, 'decay')\n",
    "# parameter_combinations = car_product.build()\n",
    "\n",
    "# historys_list = []\n",
    "# print(\"total iteration: \" + str(len(parameter_combinations)))\n",
    "# iternum = 0\n",
    "# for combination in parameter_combinations:\n",
    "#     print(\"itertion: \" + str(iternum))\n",
    "#     print(combination)\n",
    "#     model = create_model(first_units = combination['first_units'], \n",
    "#                          second_units = combination['second_units'], \n",
    "#                          dropout = combination['dropout'], \n",
    "#                          decay = combination['decay'])\n",
    "#     model_history = fit_model(model=model, batch_size=32)\n",
    "    \n",
    "#     historys_list.append({\n",
    "#         'combination': combination,\n",
    "#         'max_val_acc': max(model_history.history['val_acc'])\n",
    "#     })\n",
    "#     print(\"result: \" + str(max(model_history.history['val_acc'])))\n",
    "#     iternum += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort and output_to_file\n",
    "# ordered_history = sorted(historys_list, key= lambda x: x['max_val_acc'], reverse=True)\n",
    "\n",
    "# historys_list_dict = {\n",
    "#     \"historys\": ordered_history\n",
    "# }\n",
    "# with open('tune_hps.json', 'w') as hp_result:\n",
    "#     json.dump(ordered_history, hp_result, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11879/11879 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "# load from file\n",
    "model.load_weights(\"best_weights_head.hdf5\")\n",
    "y_test = model.predict(x_test, batch_size=128, verbose=1)\n",
    "y_test\n",
    "\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    if np.argmax(y_test[i]) == 0:\n",
    "        label = \"NOT ENOUGH INFO\"\n",
    "#         test_df['evidence'][i] = []\n",
    "    elif np.argmax(y_test[i]) == 1:\n",
    "        label = \"REFUTES\"\n",
    "    else:\n",
    "        label = \"SUPPORTS\"\n",
    "    key = test_df['key'][i]\n",
    "    result_dict.update({\n",
    "        key:{\n",
    "            \"claim\": test_df['claim'][i],\n",
    "            \"label\": label,\n",
    "            \"evidence\": test_df['evidence'][i]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "with open('result_on_dev.json', 'w') as outfile:\n",
    "    json.dump(result_dict, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train sentence by BERT features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction website: https://bert-as-service.readthedocs.io/en/latest/section/get-start.html  \n",
    "Download server and client:\n",
    "``` bash\n",
    "pip install -U bert-serving-server bert-serving-client  \n",
    "```\n",
    "Downlaod and unzip pretrained bert model(BERT-Large, Uncased, 1024 dimensional output):  \n",
    "``` bash\n",
    "cd ${model_path}\n",
    "wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
    "unzip uncased_L-24_H-1024_A-16.zip  \n",
    "```  \n",
    "\n",
    "Start bert server at local machine: \n",
    "``` bash\n",
    "bert-serving-start -model_dir ${model_path}/uncased_L-24_H-1024_A-16 -max_seq_len=100 -num_worker=1  \n",
    "bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "```\n",
    "Then, call from client end in python:\n",
    "``` python\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "bc.encode(['First do it', 'then do it right', 'then do it better'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:43:47.817994Z",
     "start_time": "2019-05-17T05:43:47.813975Z"
    }
   },
   "source": [
    "## Load data as Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T10:56:35.798961Z",
     "start_time": "2019-05-19T10:56:35.761470Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_evi_pair</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cerebral palsy has been documented throughout ...</td>\n",
       "      <td>Cerebral palsy has been documented throughout ...</td>\n",
       "      <td>Cerebral palsy CP is the most common movement ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peter Cetera was a vocalist.</td>\n",
       "      <td>Peter Cetera was a vocalist. ||| Peter Cetera ...</td>\n",
       "      <td>Peter Cetera With `` If You Leave Me Now , '' ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Judy Greer is an American.</td>\n",
       "      <td>Judy Greer is an American. ||| Judy Greer Judy...</td>\n",
       "      <td>Judy Greer Judy Greer born Judith Therese Evan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julie Christie was nominated for an Oscar for ...</td>\n",
       "      <td>Julie Christie was nominated for an Oscar for ...</td>\n",
       "      <td>Julie Christie She came to international atten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wikipedia has zero articles.</td>\n",
       "      <td>Wikipedia has zero articles. ||| Wikipedia Ove...</td>\n",
       "      <td>Wikipedia Overall , Wikipedia consists of more...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Cerebral palsy has been documented throughout ...   \n",
       "1                       Peter Cetera was a vocalist.   \n",
       "2                         Judy Greer is an American.   \n",
       "3  Julie Christie was nominated for an Oscar for ...   \n",
       "4                       Wikipedia has zero articles.   \n",
       "\n",
       "                                      claim_evi_pair  \\\n",
       "0  Cerebral palsy has been documented throughout ...   \n",
       "1  Peter Cetera was a vocalist. ||| Peter Cetera ...   \n",
       "2  Judy Greer is an American. ||| Judy Greer Judy...   \n",
       "3  Julie Christie was nominated for an Oscar for ...   \n",
       "4  Wikipedia has zero articles. ||| Wikipedia Ove...   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  Cerebral palsy CP is the most common movement ...      0  \n",
       "1  Peter Cetera With `` If You Leave Me Now , '' ...      1  \n",
       "2  Judy Greer Judy Greer born Judith Therese Evan...      1  \n",
       "3  Julie Christie She came to international atten...      0  \n",
       "4  Wikipedia Overall , Wikipedia consists of more...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./training.csv\"\n",
    "test_file_path = './dev.csv'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11982"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T10:22:02.299532Z",
     "start_time": "2019-05-17T10:22:02.296894Z"
    }
   },
   "source": [
    "### Construct and save bert features to file for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T10:22:01.605677Z",
     "start_time": "2019-05-17T08:53:06.342210Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# from bert_serving.client import BertClient\n",
    "# bc = BertClient()\n",
    "\n",
    "# for cased restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/cased_L-24_H-1024_A-16/ -cased_tokenization -max_batch_size=1024 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "# for uncased restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/cased_L-24_H-1024_A-16/ -max_batch_size=1024 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "# case = \"_cased\"\n",
    "# case = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test claim encode\n",
    "# restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "\n",
    "# train_claim_encode = bc.encode(list(train_df['claim']))\n",
    "# np.save(\"./Sentence_encodings/train_claim_encode\" + case, train_claim_encode)\n",
    "\n",
    "# test_claim_encode = bc.encode(list(test_df['claim']))\n",
    "# np.save(\"./Sentence_encodings/test_claim_encode\" + case, test_claim_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test evidence encode\n",
    "# restart server with \n",
    "\n",
    "# train_evi_encode = bc.encode(list(train_df['evidence']))\n",
    "# np.save(\"./Sentence_encodings/train_evi_encode\" + case, train_evi_encode)\n",
    "\n",
    "# test_evi_encode = bc.encode(list(test_df['evidence']))\n",
    "# np.save(\"./Sentence_encodings/test_evi_encode\" + case, test_evi_encode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train, test claim+evidence pair encode\n",
    "# restart server with \n",
    "\n",
    "\n",
    "# train_pair_encode = bc.encode(list(train_df['claim_evi_pair']))\n",
    "# np.save(\"./Sentence_encodings/train_pair_encode\" + case, train_pair_encode)\n",
    "\n",
    "# test_pair_encode = bc.encode(list(test_df['claim_evi_pair']))\n",
    "# np.save(\"./Sentence_encodings/test_pair_encode\" + case, test_pair_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bert features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_claim_features = np.load(\"./Sentence_encodings/train_claim_encode.npy\")\n",
    "# # test_claim_features = np.load(\"./Sentence_encodings/test_claim_encode.npy\")\n",
    "\n",
    "# train_evi_features = np.load(\"./Sentence_encodings/train_evi_encode.npy\")\n",
    "# # test_evi_features = np.load(\"./Sentence_encodings/test_evi_encode.npy\")\n",
    "\n",
    "# train_pair_features = np.load(\"./Sentence_encodings/train_pair_encode.npy\")\n",
    "# # test_pair_features = np.load(\"././Sentence_encodings/test_pair_encode.npy\")\n",
    "\n",
    "train_claim_features_cased = np.load(\"./Sentence_encodings/train_claim_encode_cased.npy\")\n",
    "test_claim_features_cased = np.load(\"./Sentence_encodings/test_claim_encode_cased.npy\")\n",
    "\n",
    "train_evi_features_cased = np.load(\"./Sentence_encodings/train_evi_encode_cased.npy\")\n",
    "test_evi_features_cased = np.load(\"./Sentence_encodings/test_evi_encode_cased.npy\")\n",
    "\n",
    "train_pair_features_cased = np.load(\"./Sentence_encodings/train_pair_encode_cased.npy\")\n",
    "test_pair_features_cased = np.load(\"././Sentence_encodings/test_pair_encode_cased.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.concatenate([train_claim_features, train_evi_features, train_pair_features, train_claim_features_cased, train_evi_features_cased, train_pair_features_cased], axis=1)\n",
    "x_train = np.concatenate([train_claim_features_cased, train_evi_features_cased, train_pair_features_cased], axis=1)\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "# x_test = np.concatenate([test_claim_features, test_evi_features, test_pair_features, test_claim_features_cased, test_evi_features_cased, test_pair_features_cased], axis=1\n",
    "x_test = np.concatenate([test_claim_features_cased, test_evi_features_cased, test_pair_features_cased], axis=1)\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484632, 3072)\n",
      "(484632,)\n",
      "(11982, 3072)\n",
      "(11982,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLP to train (with bert features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T10:53:32.153100Z",
     "start_time": "2019-05-19T10:53:32.070934Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 200)               614600    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 624,701\n",
      "Trainable params: 624,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 411937 samples, validate on 72695 samples\n",
      "Epoch 1/50\n",
      "411937/411937 [==============================] - 31s 76us/step - loss: 0.4810 - acc: 0.7691 - val_loss: 0.4206 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.80265, saving model to best_weights_sentence.hdf5\n",
      "Epoch 2/50\n",
      "411937/411937 [==============================] - 31s 74us/step - loss: 0.4211 - acc: 0.8057 - val_loss: 0.3955 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.80265 to 0.82202, saving model to best_weights_sentence.hdf5\n",
      "Epoch 3/50\n",
      "411937/411937 [==============================] - 31s 75us/step - loss: 0.3970 - acc: 0.8191 - val_loss: 0.3700 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.82202 to 0.83332, saving model to best_weights_sentence.hdf5\n",
      "Epoch 4/50\n",
      "411937/411937 [==============================] - 31s 74us/step - loss: 0.3814 - acc: 0.8277 - val_loss: 0.3670 - val_acc: 0.8356\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.83332 to 0.83559, saving model to best_weights_sentence.hdf5\n",
      "Epoch 5/50\n",
      "411937/411937 [==============================] - 31s 74us/step - loss: 0.3691 - acc: 0.8338 - val_loss: 0.3542 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.83559 to 0.84105, saving model to best_weights_sentence.hdf5\n",
      "Epoch 6/50\n",
      "411937/411937 [==============================] - 31s 74us/step - loss: 0.3606 - acc: 0.8388 - val_loss: 0.3434 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.84105 to 0.84908, saving model to best_weights_sentence.hdf5\n",
      "Epoch 7/50\n",
      "411937/411937 [==============================] - 31s 75us/step - loss: 0.3526 - acc: 0.8431 - val_loss: 0.3418 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84908 to 0.84934, saving model to best_weights_sentence.hdf5\n",
      "Epoch 8/50\n",
      "411937/411937 [==============================] - 31s 74us/step - loss: 0.3461 - acc: 0.8466 - val_loss: 0.3410 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.84934 to 0.85026, saving model to best_weights_sentence.hdf5\n",
      "Epoch 9/50\n",
      "411937/411937 [==============================] - 31s 74us/step - loss: 0.3399 - acc: 0.8491 - val_loss: 0.3350 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85026 to 0.85284, saving model to best_weights_sentence.hdf5\n",
      "Epoch 10/50\n",
      "411937/411937 [==============================] - 32s 77us/step - loss: 0.3355 - acc: 0.8516 - val_loss: 0.3324 - val_acc: 0.8552\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.85284 to 0.85524, saving model to best_weights_sentence.hdf5\n",
      "Epoch 11/50\n",
      "411937/411937 [==============================] - 30s 73us/step - loss: 0.3298 - acc: 0.8546 - val_loss: 0.3271 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.85524 to 0.85718, saving model to best_weights_sentence.hdf5\n",
      "Epoch 12/50\n",
      "411937/411937 [==============================] - 30s 73us/step - loss: 0.3259 - acc: 0.8559 - val_loss: 0.3280 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2fcdb3a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=200, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=50, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "# optimizer = Adam(lr=0.01)\n",
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# callbacks\n",
    "filepath=\"best_weights_sentence.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=32, epochs=50, validation_split=0.15, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11982/11982 [==============================] - 0s 10us/step\n",
      "accoracy on test set: 0.799783007845101\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"best_weights_sentence.hdf5\")\n",
    "y_test_predict = model.predict(x_test, batch_size=128, verbose=1)\n",
    "\n",
    "\n",
    "correct = 0.0\n",
    "total = len(y_test)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 1 and y_test_predict[i][0] > 0.5:\n",
    "        correct += 1\n",
    "    if y_test[i] == 0 and y_test_predict[i][0] <= 0.5:\n",
    "        correct += 1\n",
    "accuracy = correct / total\n",
    "print(\"accoracy on test set: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
